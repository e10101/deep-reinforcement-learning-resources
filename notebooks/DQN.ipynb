{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref: https://www.tensorflow.org/agents/tutorials/1_dqn_tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tf_agents.environments import suite_gym, tf_py_environment\n",
    "from tf_agents.networks import sequential, q_network\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import reverb_replay_buffer\n",
    "from tf_agents.replay_buffers import reverb_utils\n",
    "from tf_agents.drivers import py_driver\n",
    "from tf_agents.policies import py_tf_eager_policy\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.drivers import dynamic_step_driver, dynamic_episode_driver\n",
    "from tf_agents.eval import metric_utils\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import reverb\n",
    "\n",
    "import tensorflow as tf\n",
    "import pyvirtualdisplay\n",
    "\n",
    "\n",
    "import PIL\n",
    "from PIL import ImageDraw, ImageFont\n",
    "import numpy as np\n",
    "import IPython\n",
    "import imageio\n",
    "import base64\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "from absl import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.set_verbosity(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env_name = 'Taxi-v3'\n",
    "train_or_retrain = True\n",
    "num_iterations = 1_500\n",
    "max_episode_steps=200\n",
    "\n",
    "learning_rate = 0.001 # 1e-4\n",
    "num_eval_episodes = 10\n",
    "replay_buffer_max_length = 100_000\n",
    "# initial_collect_steps = 10_000\n",
    "initial_collect_episodes = 640\n",
    "\n",
    "batch_size = 64\n",
    "collect_steps_per_iteration = 1\n",
    "collect_episodes_per_iteration = 1\n",
    "\n",
    "log_interval = 50\n",
    "eval_interval = 100\n",
    "video_recording_interval = 1_000\n",
    "\n",
    "root_dir = os.path.join('./data', env_name)\n",
    "summaries_flush_secs = 10\n",
    "\n",
    "fc_layer_params = (100, 50, 50)\n",
    "gamma = 0.9\n",
    "reward_scale_factor = 1.0\n",
    "gradient_clipping = None\n",
    "debug_summaries = False\n",
    "summarize_grads_and_vars = False\n",
    "\n",
    "# Params for train\n",
    "use_tf_functions = True\n",
    "train_steps_per_iteration = max_episode_steps\n",
    "\n",
    "# Params for collect\n",
    "# epsilon_greedy = 0.1\n",
    "replay_buffer_capacity = 100_000\n",
    "\n",
    "# Params for target update\n",
    "target_update_tau = 0.1\n",
    "target_update_period = 5\n",
    "\n",
    "# Params for summaries and logging\n",
    "summary_interval = 100\n",
    "eval_metrics_callback = None\n",
    "\n",
    "train_sequence_length = 1\n",
    "\n",
    "# Params for checkpoints\n",
    "train_checkpoint_interval = 1_000\n",
    "policy_checkpoint_interval = 5000\n",
    "rb_checkpoint_interval = 2_000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = os.path.expanduser(root_dir)\n",
    "train_dir = os.path.join(root_dir, 'train')\n",
    "eval_dir = os.path.join(root_dir, 'eval')\n",
    "video_dir = os.path.join(root_dir, 'video')\n",
    "\n",
    "# Create the video recording directory\n",
    "os.makedirs(video_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Summary Writer\n",
    "train_summary_writer = tf.summary.create_file_writer(\n",
    "    train_dir, flush_millis=summaries_flush_secs * 1000)\n",
    "train_summary_writer.set_as_default()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval Summary Writer\n",
    "eval_summary_writer = tf.summary.create_file_writer(\n",
    "    eval_dir, flush_millis=summaries_flush_secs * 1000)\n",
    "eval_metrics = [\n",
    "    tf_metrics.AverageReturnMetric(buffer_size=num_eval_episodes),\n",
    "    tf_metrics.AverageEpisodeLengthMetric(buffer_size=num_eval_episodes)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create global_step\n",
    "global_step = tf.compat.v1.train.get_or_create_global_step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_q_net(tf_env):\n",
    "    obervation_space_size = tf_env.observation_spec().maximum - tf_env.observation_spec().minimum + 1\n",
    "    action_space_size = tf_env.action_spec().maximum - tf_env.action_spec().minimum + 1\n",
    "\n",
    "    # obervation_space_size, action_space_size\n",
    "    \n",
    "    q_net = sequential.Sequential([\n",
    "        tf.keras.layers.Embedding(obervation_space_size, 10, input_length=1),\n",
    "        tf.keras.layers.Reshape((10,)),\n",
    "        tf.keras.layers.Dense(50, activation='relu'),\n",
    "        tf.keras.layers.Dense(50, activation='relu'),\n",
    "        tf.keras.layers.Dense(action_space_size, activation='linear'),\n",
    "    ])\n",
    "    \n",
    "    return q_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_q_net_v2(tf_env):\n",
    "    obervation_space_size = tf_env.observation_spec().maximum - tf_env.observation_spec().minimum + 1\n",
    "    action_space_size = tf_env.action_spec().maximum - tf_env.action_spec().minimum + 1\n",
    "\n",
    "    # obervation_space_size, action_space_size\n",
    "    \n",
    "    q_net = sequential.Sequential([\n",
    "        tf.keras.layers.Embedding(obervation_space_size, action_space_size, input_length=1),\n",
    "        # tf.keras.layers.Reshape((10,)),\n",
    "        # tf.keras.layers.Dense(50, activation='relu'),\n",
    "        # tf.keras.layers.Dense(50, activation='relu'),\n",
    "        # tf.keras.layers.Dense(action_space_size, activation='linear'),\n",
    "    ])\n",
    "    \n",
    "    return q_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_logger(_):\n",
    "    print('global_step', global_step, _)\n",
    "    \n",
    "    return _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_func(global_step):\n",
    "    epsilon_decay = 0.0001\n",
    "    epsilon_min = 0.001\n",
    "    \n",
    "    steps = 0\n",
    "    \n",
    "    def value():\n",
    "        val = epsilon_min + (1 - epsilon_min) * tf.exp(tf.cast(global_step, dtype=tf.float32) * -1 * epsilon_decay)\n",
    "        \n",
    "        \n",
    "        tf.compat.v2.summary.scalar(\n",
    "                name='epsilon', data=val, step=global_step)\n",
    "        \n",
    "        return val\n",
    "    \n",
    "    \n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tf_env = tf_py_environment.TFPyEnvironment(suite_gym.load(env_name, max_episode_steps=max_episode_steps))\n",
    "eval_py_env = suite_gym.load(env_name, max_episode_steps=max_episode_steps)\n",
    "eval_tf_env = tf_py_environment.TFPyEnvironment(eval_py_env)\n",
    "\n",
    "random_policy = random_tf_policy.RandomTFPolicy(\n",
    "    tf_env.time_step_spec(),\n",
    "    tf_env.action_spec(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with tf.summary.record_if(lambda: tf.math.equal(global_step % summary_interval, 0)):\n",
    "    # Create env\n",
    "    tf_env = tf_py_environment.TFPyEnvironment(suite_gym.load(env_name, max_episode_steps=max_episode_steps))\n",
    "    eval_py_env = suite_gym.load(env_name, max_episode_steps=max_episode_steps)\n",
    "    eval_tf_env = tf_py_environment.TFPyEnvironment(eval_py_env)\n",
    "\n",
    "    q_net = get_q_net_v2(tf_env)\n",
    "    \n",
    "    # Create Agent\n",
    "    tf_agent = dqn_agent.DqnAgent(\n",
    "        time_step_spec=tf_env.time_step_spec(),\n",
    "        action_spec=tf_env.action_spec(),\n",
    "        q_network=q_net,\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        \n",
    "        # Params for collect\n",
    "        # epsilon_greedy=epsilon_greedy,\n",
    "        epsilon_greedy=epsilon_greedy_func(global_step),\n",
    "        \n",
    "        # Params for target network updates\n",
    "        target_q_network=None,\n",
    "        target_update_tau=target_update_tau,  # Default: 1.0, \"Factor for soft update of the target network\"\n",
    "        target_update_period=target_update_period,  # Default: 1, \"Period for soft update of the target network\"\n",
    "        \n",
    "        # Params for training\n",
    "        td_errors_loss_fn=common.element_wise_squared_loss,  # Default: common.element_wise_huber_loss\n",
    "        gamma=gamma,  # Default: 1.0, Discount for future rewards.\n",
    "        reward_scale_factor=reward_scale_factor,  # Default: 1.0\n",
    "        gradient_clipping=gradient_clipping,  # Default: None, \"Norm length to clip gradients\"\n",
    "        \n",
    "        # Params for debugging\n",
    "        train_step_counter=global_step,\n",
    "        debug_summaries=debug_summaries,\n",
    "        summarize_grads_and_vars=summarize_grads_and_vars,\n",
    "        name=None,  # Default: class name. The agent name.\n",
    "    )\n",
    "    tf_agent.initialize()\n",
    "    \n",
    "    # Train Metrics\n",
    "    train_metrics = [\n",
    "        tf_metrics.NumberOfEpisodes(),\n",
    "        tf_metrics.EnvironmentSteps(),\n",
    "        tf_metrics.AverageReturnMetric(),\n",
    "        tf_metrics.AverageEpisodeLengthMetric(),\n",
    "    ]\n",
    "    \n",
    "    # Policies\n",
    "    eval_policy = tf_agent.policy\n",
    "    collect_policy = tf_agent.collect_policy\n",
    "    \n",
    "    # Replay Buffer\n",
    "    replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "        data_spec=tf_agent.collect_data_spec,\n",
    "        batch_size=tf_env.batch_size,\n",
    "        max_length=replay_buffer_capacity)\n",
    "    \n",
    "    # Collect Driver\n",
    "    collect_driver = dynamic_episode_driver.DynamicEpisodeDriver(\n",
    "        tf_env,\n",
    "        collect_policy,\n",
    "        observers=[replay_buffer.add_batch] + train_metrics,\n",
    "        num_episodes=collect_episodes_per_iteration)\n",
    "\n",
    "    # Speed up with common.function\n",
    "    if use_tf_functions:\n",
    "        collect_driver.run = common.function(collect_driver.run)\n",
    "        tf_agent.train = common.function(tf_agent.train)\n",
    "\n",
    "        # dynamic_episode_driver.run = common.function(dynamic_episode_driver.run)\n",
    "\n",
    "        ############################################\n",
    "    print(\"#1\")\n",
    "    # Collect initial replay buffer data.\n",
    "\n",
    "    initial_collect_policy = random_tf_policy.RandomTFPolicy(\n",
    "        tf_env.time_step_spec(),\n",
    "        tf_env.action_spec(),\n",
    "    )\n",
    "\n",
    "    logging.info(\n",
    "        'Initializing replay buffer by collecting experience for %d episodes with '\n",
    "        'a random policy.', initial_collect_episodes)\n",
    "    \n",
    "    \n",
    "\n",
    "    initial_driver = dynamic_episode_driver.DynamicEpisodeDriver(\n",
    "        tf_env,\n",
    "        initial_collect_policy,\n",
    "        observers=[replay_buffer.add_batch] + train_metrics + [initial_logger],\n",
    "        num_episodes=initial_collect_episodes)\n",
    "\n",
    "    if use_tf_functions:\n",
    "        initial_driver.run = common.function(initial_driver.run)\n",
    "    initial_driver.run()\n",
    "    \n",
    "    \n",
    "    ##################################################\n",
    "    print(\"#2\")\n",
    "\n",
    "    results = metric_utils.eager_compute(\n",
    "        eval_metrics,\n",
    "        eval_tf_env,\n",
    "        eval_policy,\n",
    "        num_episodes=num_eval_episodes,\n",
    "        train_step=global_step,\n",
    "        summary_writer=eval_summary_writer,\n",
    "        summary_prefix='Metrics',\n",
    "    )\n",
    "    if eval_metrics_callback is not None:\n",
    "        eval_metrics_callback(results, global_step.numpy())\n",
    "    metric_utils.log_metrics(eval_metrics)\n",
    "\n",
    "    time_step = None\n",
    "    policy_state = collect_policy.get_initial_state(tf_env.batch_size)\n",
    "\n",
    "    # Variables for logging time (steps_per_sec)\n",
    "    timed_at_step = global_step.numpy()\n",
    "    time_acc = 0  # Time accumulation\n",
    "\n",
    "    # Dataset\n",
    "    dataset = replay_buffer.as_dataset(\n",
    "        num_parallel_calls=3,\n",
    "        sample_batch_size=batch_size,\n",
    "        num_steps=train_sequence_length + 1,\n",
    "        single_deterministic_pass=False,\n",
    "    ).prefetch(3)\n",
    "    iterator = iter(dataset)\n",
    "\n",
    "    def train_step():\n",
    "        experience, _ = next(iterator)\n",
    "        return tf_agent.train(experience)\n",
    "\n",
    "    if use_tf_functions:\n",
    "        train_step = common.function(train_step)\n",
    "    \n",
    "    #############################\n",
    "    print(\"#3\")\n",
    "\n",
    "    for _ in range(num_iterations):\n",
    "        start_time = time.time()\n",
    "        time_step, policy_state = collect_driver.run(\n",
    "            time_step=time_step,\n",
    "            policy_state=policy_state,\n",
    "        )\n",
    "\n",
    "        for _ in range(train_steps_per_iteration):\n",
    "            train_loss = train_step()\n",
    "        time_acc += time.time() - start_time\n",
    "\n",
    "        if global_step.numpy() % log_interval == 0:\n",
    "            logging.info('step = %d, loss = %f', global_step.numpy(),\n",
    "                         train_loss.loss)\n",
    "            steps_per_sec = (global_step.numpy() - timed_at_step) / time_acc\n",
    "            logging.info('%.3f steps/sec', steps_per_sec)\n",
    "            tf.compat.v2.summary.scalar(\n",
    "                name='global_steps_per_sec', data=steps_per_sec, step=global_step)\n",
    "\n",
    "            # Reset time.\n",
    "            timed_at_step = global_step.numpy()\n",
    "            time_acc = 0\n",
    "\n",
    "        for train_metric in train_metrics:\n",
    "            train_metric.tf_summaries(\n",
    "                train_step=global_step, step_metrics=train_metrics[:2])\n",
    "\n",
    "        if global_step.numpy() % eval_interval == 0:\n",
    "            results = metric_utils.eager_compute(\n",
    "                eval_metrics,\n",
    "                eval_tf_env,\n",
    "                eval_policy,\n",
    "                num_episodes=num_eval_episodes,\n",
    "                train_step=global_step,\n",
    "                summary_writer=eval_summary_writer,\n",
    "                summary_prefix='Metrics',\n",
    "            )\n",
    "            if eval_metrics_callback is not None:\n",
    "                eval_metrics_callback(results, global_step.numpy())\n",
    "            metric_utils.log_metrics(eval_metrics)\n",
    "\n",
    "        # # Record a video of current eval agent policy\n",
    "        # if global_step.numpy() % video_recording_interval == 0:\n",
    "        #     filename = '{}_{}'.format(global_step.numpy(), get_timestamp())\n",
    "        #     full_filename = os.path.join(video_dir, filename)\n",
    "        #     create_policy_eval_video(\n",
    "        #         eval_policy,\n",
    "        #         eval_tf_env,\n",
    "        #         eval_py_env,\n",
    "        #         filename=full_filename,\n",
    "        #         fps=15,\n",
    "        #         freeze_seconds=3,\n",
    "        #         num_episodes=1,\n",
    "        #         step=global_step.numpy(),\n",
    "        #     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saved_filename = create_policy_eval_video(tf_agent.policy, eval_tf_env, eval_py_env, fps=15, freeze_seconds=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed_mp4(saved_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf_agents.policies.greedy_policy.GreedyPolicy at 0x7fb4a53b92e0>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_agent.policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_env(eval_tf_env, policy, sleep_time=0.1, env_seed=None, max_steps=200):\n",
    "\n",
    "    def get_state_and_reward_from_time_step(ts):\n",
    "        return time_step.observation[0].numpy(), time_step.reward[0].numpy()\n",
    "\n",
    "    states = []\n",
    "    rewards = []\n",
    "    actions = []\n",
    "    \n",
    "    time_step = eval_tf_env.reset()\n",
    "    state, reward = get_state_and_reward_from_time_step(time_step)\n",
    "    \n",
    "    states.append(state)\n",
    "    rewards.append(reward)\n",
    "    actions.append(None)\n",
    "    \n",
    "    total_reward = 0\n",
    "    is_done = False\n",
    "    current_step = 0\n",
    "\n",
    "    while not time_step.is_last():\n",
    "        clear_output(wait=True)\n",
    "        # Get a random action\n",
    "        action_step = policy.action(time_step)\n",
    "        time_step = eval_tf_env.step(action_step.action)\n",
    "        \n",
    "        print(time_step)\n",
    "        \n",
    "        state, reward = get_state_and_reward_from_time_step(time_step)\n",
    "        action = action_step.action[0].numpy()\n",
    "\n",
    "        states.append(state)\n",
    "        rewards.append(reward)\n",
    "        actions.append(action)\n",
    "\n",
    "        total_reward += reward\n",
    "        current_step += 1\n",
    "\n",
    "\n",
    "        # Print header\n",
    "        print('Step: {:03d}, Reward: {}\\n'.format(\n",
    "            current_step,\n",
    "            total_reward,\n",
    "        ))\n",
    "        \n",
    "\n",
    "        time.sleep(sleep_time)\n",
    "        \n",
    "        \n",
    "    if current_step < max_steps:\n",
    "        print('\\nResult: Done with {} steps and total reward is {}.'.format(\n",
    "            current_step,\n",
    "            total_reward,\n",
    "        ))\n",
    "    else:\n",
    "        print('\\nResult: Unsolved')\n",
    "        \n",
    "    return states, rewards, actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeStep(\n",
      "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>,\n",
      " 'observation': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([85])>,\n",
      " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([20.], dtype=float32)>,\n",
      " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2], dtype=int32)>})\n",
      "Step: 007, Reward: 14.0\n",
      "\n",
      "\n",
      "Result: Done with 7 steps and total reward is 14.0.\n"
     ]
    }
   ],
   "source": [
    "states, rewards, actions = play_env(eval_tf_env, tf_agent.policy, max_steps=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_char_txt(char_row, char_col, char='█'):\n",
    "    txt = ''\n",
    "    for r in range(char_row+2):\n",
    "        for c in range(char_col * 2 + 2):\n",
    "            if (char_row + 1) == r and (char_col * 2 + 1) == c:\n",
    "                txt += char\n",
    "            else:\n",
    "                txt += ' '\n",
    "        txt += '\\n'\n",
    "    \n",
    "    return txt\n",
    "\n",
    "def get_char_by_index(idx: int):\n",
    "    if idx == 0:\n",
    "        return 'R'\n",
    "    elif idx == 1:\n",
    "        return 'G'\n",
    "    elif idx == 2:\n",
    "        return 'Y'\n",
    "    elif idx == 3:\n",
    "        return 'B'\n",
    "    elif idx == 4:\n",
    "        return '_'\n",
    "    \n",
    "    return ' '\n",
    "\n",
    "def get_char_pos_by_index(idx: int, taxi_row: int, taxi_col: int):\n",
    "    if idx == 0:\n",
    "        return [0, 0]\n",
    "    elif idx == 1:\n",
    "        return [0, 4]\n",
    "    elif idx == 2:\n",
    "        return [4, 0]\n",
    "    elif idx == 3:\n",
    "        return [4, 3]\n",
    "    \n",
    "    return [taxi_row, taxi_col]\n",
    "\n",
    "def get_char_color_by_index(idx: int):\n",
    "    if idx == 0:\n",
    "        return (255, 0, 0) # Red\n",
    "    elif idx == 1:\n",
    "        return (0, 255, 0) # Green\n",
    "    elif idx == 2:\n",
    "        return (255, 255, 0) # Yellow\n",
    "    elif idx == 3:\n",
    "        return (0, 0, 255) # Blue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhance_frame(frame: np.ndarray, main_text=None, state=None, side_text=None, done=False) -> np.ndarray:\n",
    "    if main_text is None:\n",
    "        return frame\n",
    "    \n",
    "    # Convert array to PIl.Image\n",
    "    image = PIL.Image.fromarray(frame).convert('RGB')\n",
    "\n",
    "    # Get draw context\n",
    "    draw = ImageDraw.Draw(image, 'RGB')\n",
    "\n",
    "    # Get font\n",
    "    font_file = '/usr/share/fonts/truetype/dejavu/DejaVuSansMono.ttf'\n",
    "    font = ImageFont.truetype(font_file, 24)\n",
    "    \n",
    "    # Variables\n",
    "    draw_offset = (30, 30)\n",
    "    side_offset = (220, 30)\n",
    "    side_font_size = 20\n",
    "    side_font = ImageFont.truetype(font_file, side_font_size)\n",
    "    taxi_color = (255, 255, 0)\n",
    "    passenger_color = (0, 0, 255)\n",
    "    dest_color = (255, 0, 255)\n",
    "    taxi_with_passenger_color = (0, 255, 0)\n",
    "    \n",
    "    # Render state\n",
    "    if state is not None:\n",
    "        # Draw taxi (color background)\n",
    "        [taxi_row, taxi_col, passenger_location, destination] = list(env.decode(state))\n",
    "        print([taxi_row, taxi_col, passenger_location, destination])\n",
    "        taxi_txt = get_char_txt(taxi_row, taxi_col)\n",
    "        \n",
    "        taxi_color = taxi_with_passenger_color if (passenger_location == 4 and not done) else taxi_color \n",
    "        draw.text(draw_offset, taxi_txt, font=font, fill=taxi_color, stroke_width=1, stroke_fill=(100, 100, 100))\n",
    "        \n",
    "        # Draw map\n",
    "        draw.text(draw_offset, main_text, font=font, fill=(0, 0, 0), stroke_width=1, stroke_fill=(255, 255, 255))\n",
    "        \n",
    "        # Draw passenger\n",
    "        passenger_char = get_char_by_index(passenger_location)\n",
    "        [passenger_row, passenger_col] = get_char_pos_by_index(passenger_location, taxi_row, taxi_col)\n",
    "        passenger_txt = get_char_txt(passenger_row, passenger_col, char=passenger_char)\n",
    "        # passenger_color = get_char_color_by_index(passenger_location)\n",
    "        # print('passenger_txt', passenger_txt)\n",
    "        draw.text(draw_offset, passenger_txt, font=font, fill=passenger_color, stroke_width=1, stroke_fill=(255, 255, 255))\n",
    "        \n",
    "        # Draw destination\n",
    "        dest_char = get_char_by_index(destination)\n",
    "        [dest_row, dest_col] = get_char_pos_by_index(destination, taxi_row, taxi_col)\n",
    "        dest_txt = get_char_txt(dest_row, dest_col, char=dest_char)\n",
    "        # dest_color = get_char_color_by_index(destination)\n",
    "        # print('dest_txt', dest_txt)\n",
    "        draw.text(draw_offset, dest_txt, font=font, fill=dest_color, stroke_width=1, stroke_fill=(255, 255, 255))\n",
    "        \n",
    "    else:\n",
    "        # Draw background\n",
    "        draw.text(draw_offset, main_text, font=font, fill=(0, 0, 0), stroke_width=1, stroke_fill=(255, 255, 255))\n",
    "    \n",
    "    if side_text is not None:\n",
    "        draw.text(side_offset, side_text, font=side_font, fill=(0, 0, 0), stroke_width=1, stroke_fill=(255, 255, 255))\n",
    "\n",
    "    return np.array(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'Taxi-v3'\n",
    "def create_states_video(\n",
    "    states, rewards, filename=None, fps=30, \n",
    "    env_name=env_name, freeze_seconds=0, freeze_begin_seconds=0, step=None):\n",
    "    if filename is None:\n",
    "        filename = str(get_timestamp())\n",
    "        \n",
    "    filename = filename + '.mp4'\n",
    "    logging.info('Env: %s', env_name)\n",
    "    logging.info('Filename: %s', filename)\n",
    "    map_txt = '\\n'.join(taxi_map)\n",
    "\n",
    "    with imageio.get_writer(filename, fps=fps) as video:\n",
    "        logging.info('Begin')\n",
    "        total_reward = 0.0\n",
    "        frame_idx = 0\n",
    "                \n",
    "        for idx, (state, reward, action) in enumerate(zip(states, rewards, actions)):\n",
    "            done = reward == 20\n",
    "            # Freeze frame for a few seconds - At beginning\n",
    "            if idx == 0 and freeze_begin_seconds > 0:\n",
    "                text = f'Env: {env_name}'\n",
    "                if step is not None:\n",
    "                    text += f'\\nStp: {step}'\n",
    "                text += f'\\nFrm: {frame_idx}'\n",
    "                text += f'\\nRw:  {total_reward:.2f}'\n",
    "\n",
    "                frame = np.full((270, 480), 240.0)\n",
    "                frame = enhance_frame(frame, '{}'.format(map_txt), side_text=text, state=state)\n",
    "\n",
    "                for _ in range(fps * freeze_begin_seconds):\n",
    "                    video.append_data(frame)\n",
    "\n",
    "            if action is not None:\n",
    "                action_name = action_names[action]\n",
    "            else:\n",
    "                action_name = '--'\n",
    "            \n",
    "            total_reward += reward\n",
    "            \n",
    "            text = f'Env: {env_name}'\n",
    "            if step is not None:\n",
    "                text += f'\\nStp: {step}'\n",
    "            text += f'\\nFrm: {frame_idx}'\n",
    "            text += f'\\nRw:  {total_reward:.2f}'\n",
    "            text += f'\\nAct: {action_name}'\n",
    "            \n",
    "            if done:\n",
    "                text += f'\\n\\nDone!\\nFeb 26, 2022'\n",
    "\n",
    "            frame = np.full((270, 480), 240.0)\n",
    "            frame = enhance_frame(frame, '{}'.format(map_txt), side_text=text, state=state, done=done)\n",
    "            \n",
    "            video.append_data(frame)\n",
    "            \n",
    "            frame_idx += 1\n",
    "            \n",
    "            # Freeze frame for a few seconds\n",
    "            if frame_idx+1 > len(states) and freeze_seconds > 0:\n",
    "                for _ in range(fps * freeze_seconds):\n",
    "                    video.append_data(frame)\n",
    "\n",
    "    logging.info('All done')\n",
    "    return filename\n",
    "    # return embed_mp4(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.envs.toy_text.taxi import MAP as taxi_map\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Taxi-v3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_names = [\n",
    "    'South (↓)',\n",
    "    'North (↑)',\n",
    "    'East (→)',\n",
    "    'West (←)',\n",
    "    'Pickup',\n",
    "    'Drop off',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeStep(\n",
      "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>,\n",
      " 'observation': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([85])>,\n",
      " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([20.], dtype=float32)>,\n",
      " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2], dtype=int32)>})\n",
      "Step: 016, Reward: 5.0\n",
      "\n",
      "\n",
      "Result: Done with 16 steps and total reward is 5.0.\n"
     ]
    }
   ],
   "source": [
    "states, rewards, actions = play_env(eval_tf_env, tf_agent.policy, max_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Env: Taxi-v3\n",
      "INFO:absl:Filename: taxi.mp4\n",
      "INFO:absl:Begin\n",
      "WARNING:imageio_ffmpeg:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (480, 270) to (480, 272) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 0, 1]\n",
      "[2, 4, 0, 1]\n",
      "[2, 3, 0, 1]\n",
      "[2, 2, 0, 1]\n",
      "[2, 1, 0, 1]\n",
      "[1, 1, 0, 1]\n",
      "[0, 1, 0, 1]\n",
      "[0, 0, 0, 1]\n",
      "[0, 0, 4, 1]\n",
      "[1, 0, 4, 1]\n",
      "[1, 1, 4, 1]\n",
      "[2, 1, 4, 1]\n",
      "[2, 2, 4, 1]\n",
      "[2, 3, 4, 1]\n",
      "[1, 3, 4, 1]\n",
      "[1, 4, 4, 1]\n",
      "[0, 4, 4, 1]\n",
      "[0, 4, 1, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:All done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'taxi.mp4'"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_states_video(states, rewards, filename='taxi', fps=2, env_name='Taxi-v3', freeze_seconds=3, freeze_begin_seconds=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}