{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref: https://www.tensorflow.org/agents/tutorials/1_dqn_tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tf_agents.environments import suite_gym, tf_py_environment\n",
    "from tf_agents.networks import sequential, q_network\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import reverb_replay_buffer\n",
    "from tf_agents.replay_buffers import reverb_utils\n",
    "from tf_agents.drivers import py_driver\n",
    "from tf_agents.policies import py_tf_eager_policy\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.eval import metric_utils\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import reverb\n",
    "\n",
    "import tensorflow as tf\n",
    "import pyvirtualdisplay\n",
    "\n",
    "\n",
    "import PIL\n",
    "from PIL import ImageDraw, ImageFont\n",
    "import numpy as np\n",
    "import IPython\n",
    "import imageio\n",
    "import base64\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "from absl import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.set_verbosity(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env_name = 'CartPole-v0'\n",
    "env_name = 'CartPole-v1'\n",
    "env_name = 'Acrobot-v1'\n",
    "train_or_retrain = True\n",
    "num_iterations = 100_000\n",
    "\n",
    "learning_rate = 1e-4\n",
    "num_eval_episodes = 10\n",
    "replay_buffer_max_length = 100000\n",
    "initial_collect_steps = 100\n",
    "batch_size = 64\n",
    "collect_steps_per_iteration = 1\n",
    "\n",
    "log_interval = 200\n",
    "eval_interval = 1000\n",
    "video_recording_interval = 1_000\n",
    "\n",
    "root_dir = os.path.join('./data', env_name)\n",
    "summaries_flush_secs = 10\n",
    "\n",
    "fc_layer_params = (100, 50)\n",
    "gamma = 0.99\n",
    "reward_scale_factor = 1.0\n",
    "gradient_clipping = None\n",
    "debug_summaries = False\n",
    "summarize_grads_and_vars = False\n",
    "\n",
    "# Params for train\n",
    "use_tf_functions = True\n",
    "train_steps_per_iteration = 1\n",
    "\n",
    "# Params for collect\n",
    "epsilon_greedy = 0.1\n",
    "replay_buffer_capacity = 100_000\n",
    "\n",
    "# Params for target update\n",
    "target_update_tau = 0.05\n",
    "target_update_period = 5\n",
    "\n",
    "# Params for summaries and logging\n",
    "summary_interval = 1_000\n",
    "eval_metrics_callback = None\n",
    "\n",
    "train_sequence_length = 1\n",
    "\n",
    "# Params for checkpoints\n",
    "train_checkpoint_interval = 10_000\n",
    "policy_checkpoint_interval = 5_000\n",
    "rb_checkpoint_interval = 20_000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = os.path.expanduser(root_dir)\n",
    "train_dir = os.path.join(root_dir, 'train')\n",
    "eval_dir = os.path.join(root_dir, 'eval')\n",
    "video_dir = os.path.join(root_dir, 'video')\n",
    "\n",
    "# Create the video recording directory\n",
    "os.makedirs(video_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Summary Writer\n",
    "train_summary_writer = tf.summary.create_file_writer(\n",
    "    train_dir, flush_millis=summaries_flush_secs * 1000)\n",
    "train_summary_writer.set_as_default()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval Summary Writer\n",
    "eval_summary_writer = tf.summary.create_file_writer(\n",
    "    eval_dir, flush_millis=summaries_flush_secs * 1000)\n",
    "eval_metrics = [\n",
    "    tf_metrics.AverageReturnMetric(buffer_size=num_eval_episodes),\n",
    "    tf_metrics.AverageEpisodeLengthMetric(buffer_size=num_eval_episodes)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create global_step\n",
    "global_step = tf.compat.v1.train.get_or_create_global_step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timestamp():\n",
    "    import datetime\n",
    "    return datetime.datetime.now().timestamp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_mp4(filename):\n",
    "    video = open(filename, 'rb').read()\n",
    "    b64 = base64.b64encode(video)\n",
    "    \n",
    "    tag = '''\n",
    "    <video width=\"640\" height=\"480\" controls>\n",
    "        <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\">\n",
    "    Your browser does not support the video tag.\n",
    "    </video>'''.format(b64.decode())\n",
    "    \n",
    "    return IPython.display.HTML(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhance_frame(frame: np.ndarray, text=None) -> np.ndarray:\n",
    "    if text is None:\n",
    "        return frame\n",
    "    \n",
    "    # Convert array to PIl.Image\n",
    "    image = PIL.Image.fromarray(frame).convert('RGB')\n",
    "\n",
    "    # Get draw context\n",
    "    draw = ImageDraw.Draw(image, 'RGB')\n",
    "\n",
    "    # Get font\n",
    "    font = ImageFont.truetype('/usr/share/fonts/truetype/dejavu/DejaVuSansMono.ttf', 20)\n",
    "\n",
    "    # Draw text\n",
    "    draw.text((30, 30), text, font=font, fill=(0, 0, 0), stroke_width=1, stroke_fill=(255, 255, 255))\n",
    "\n",
    "    return np.array(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_policy_eval_video(policy, eval_env, eval_py_env, filename=None, \n",
    "        num_episodes=3, fps=30, env_name=env_name, freeze_seconds=0,\n",
    "        step=None):\n",
    "    if filename is None:\n",
    "        filename = str(get_timestamp())\n",
    "        \n",
    "    filename = filename + '.mp4'\n",
    "    logging.info('Env: %s', env_name)\n",
    "    logging.info('Filename: %s', filename)\n",
    "    \n",
    "    with imageio.get_writer(filename, fps=fps) as video:\n",
    "        for idx in range(num_episodes):\n",
    "            logging.info('Begin #%d of %d', idx+1, num_episodes)\n",
    "            time_step = eval_env.reset()\n",
    "            frame_idx = 0\n",
    "            \n",
    "            text = f'Env: {env_name}'\n",
    "            if step is not None:\n",
    "                text += f'\\nStp: {step}'\n",
    "            text += f'\\nEp:  {idx+1}/{num_episodes}\\nFrm: {frame_idx}'\n",
    "            \n",
    "            frame = enhance_frame(eval_py_env.render(mode='rgb_array'), text)\n",
    "            video.append_data(frame)\n",
    "\n",
    "            while not time_step.is_last():\n",
    "                action_step = policy.action(time_step)\n",
    "                time_step = eval_env.step(action_step.action)\n",
    "                frame_idx += 1\n",
    "                \n",
    "                text = f'Env: {env_name}'\n",
    "                if step is not None:\n",
    "                    text += f'\\nStp: {step}'\n",
    "                text += f'\\nEp:  {idx+1}/{num_episodes}\\nFrm: {frame_idx}'\n",
    "\n",
    "                frame = enhance_frame(eval_py_env.render(mode='rgb_array'), text)\n",
    "                video.append_data(frame)\n",
    "                \n",
    "                # Freeze frame for a few seconds\n",
    "                if time_step.is_last() and freeze_seconds > 0:\n",
    "                    for _ in range(fps * freeze_seconds):\n",
    "                        video.append_data(frame)\n",
    "    \n",
    "    logging.info('All done')\n",
    "    return filename\n",
    "    # return embed_mp4(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with tf.summary.record_if(lambda: tf.math.equal(global_step % summary_interval, 0)):\n",
    "    # Create env\n",
    "    tf_env = tf_py_environment.TFPyEnvironment(suite_gym.load(env_name))\n",
    "    eval_py_env = suite_gym.load(env_name)\n",
    "    eval_tf_env = tf_py_environment.TFPyEnvironment(eval_py_env)\n",
    "    \n",
    "    # Create Q network\n",
    "    q_net = q_network.QNetwork(\n",
    "        tf_env.observation_spec(),\n",
    "        tf_env.action_spec(),\n",
    "        fc_layer_params=fc_layer_params)\n",
    "    \n",
    "    # Create Agent\n",
    "    tf_agent = dqn_agent.DqnAgent(\n",
    "        time_step_spec=tf_env.time_step_spec(),\n",
    "        action_spec=tf_env.action_spec(),\n",
    "        q_network=q_net,\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        \n",
    "        # Params for collect\n",
    "        epsilon_greedy=epsilon_greedy,\n",
    "        \n",
    "        # Params for target network updates\n",
    "        target_q_network=None,\n",
    "        target_update_tau=target_update_tau,  # Default: 1.0, \"Factor for soft update of the target network\"\n",
    "        target_update_period=target_update_period,  # Default: 1, \"Period for soft update of the target network\"\n",
    "        \n",
    "        # Params for training\n",
    "        td_errors_loss_fn=common.element_wise_squared_loss,  # Default: common.element_wise_huber_loss\n",
    "        gamma=gamma,  # Default: 1.0, Discount for future rewards.\n",
    "        reward_scale_factor=reward_scale_factor,  # Default: 1.0\n",
    "        gradient_clipping=gradient_clipping,  # Default: None, \"Norm length to clip gradients\"\n",
    "        \n",
    "        # Params for debugging\n",
    "        train_step_counter=global_step,\n",
    "        debug_summaries=debug_summaries,\n",
    "        summarize_grads_and_vars=summarize_grads_and_vars,\n",
    "        name=None,  # Default: class name. The agent name.\n",
    "    )\n",
    "    tf_agent.initialize()\n",
    "    \n",
    "    # Train Metrics\n",
    "    train_metrics = [\n",
    "        tf_metrics.NumberOfEpisodes(),\n",
    "        tf_metrics.EnvironmentSteps(),\n",
    "        tf_metrics.AverageReturnMetric(),\n",
    "        tf_metrics.AverageEpisodeLengthMetric(),\n",
    "    ]\n",
    "    \n",
    "    # Policies\n",
    "    eval_policy = tf_agent.policy\n",
    "    collect_policy = tf_agent.collect_policy\n",
    "    \n",
    "    # Replay Buffer\n",
    "    replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "        data_spec=tf_agent.collect_data_spec,\n",
    "        batch_size=tf_env.batch_size,\n",
    "        max_length=replay_buffer_capacity)\n",
    "    \n",
    "    # Collect Driver\n",
    "    collect_driver = dynamic_step_driver.DynamicStepDriver(\n",
    "        tf_env,\n",
    "        collect_policy,\n",
    "        observers=[replay_buffer.add_batch] + train_metrics,\n",
    "        num_steps=collect_steps_per_iteration)\n",
    "    \n",
    "    # Checkpointers\n",
    "    train_checkpointer = common.Checkpointer(\n",
    "        ckpt_dir=train_dir,\n",
    "        agent=tf_agent,\n",
    "        global_step=global_step,\n",
    "        metrics=metric_utils.MetricsGroup(train_metrics, 'train_metrics'))\n",
    "    policy_checkpointer = common.Checkpointer(\n",
    "        ckpt_dir=os.path.join(train_dir, 'policy'),\n",
    "        policy=eval_policy,\n",
    "        global_step=global_step)\n",
    "    rb_checkpointer = common.Checkpointer(\n",
    "        ckpt_dir=os.path.join(train_dir, 'replay_buffer'),\n",
    "        max_to_keep=1,\n",
    "        replay_buffer=replay_buffer)\n",
    "    \n",
    "    train_checkpointer.initialize_or_restore()\n",
    "    policy_checkpointer.initialize_or_restore()\n",
    "    rb_checkpointer.initialize_or_restore()\n",
    "    \n",
    "    if train_or_retrain:\n",
    "\n",
    "        # Speed up with common.function\n",
    "        if use_tf_functions:\n",
    "            collect_driver.run = common.function(collect_driver.run)\n",
    "            tf_agent.train = common.function(tf_agent.train)\n",
    "\n",
    "        # Collect initial replay buffer data.\n",
    "\n",
    "        initial_collect_policy = random_tf_policy.RandomTFPolicy(\n",
    "            tf_env.time_step_spec(),\n",
    "            tf_env.action_spec(),\n",
    "        )\n",
    "\n",
    "        logging.info(\n",
    "            'Initializing replay buffer by collecting experience for %d steps with '\n",
    "            'a random policy.', initial_collect_steps)\n",
    "\n",
    "        dynamic_step_driver.DynamicStepDriver(\n",
    "            tf_env,\n",
    "            initial_collect_policy,\n",
    "            observers=[replay_buffer.add_batch] + train_metrics,\n",
    "            num_steps=initial_collect_steps).run()\n",
    "\n",
    "        results = metric_utils.eager_compute(\n",
    "            eval_metrics,\n",
    "            eval_tf_env,\n",
    "            eval_policy,\n",
    "            num_episodes=num_eval_episodes,\n",
    "            train_step=global_step,\n",
    "            summary_writer=eval_summary_writer,\n",
    "            summary_prefix='Metrics',\n",
    "        )\n",
    "        if eval_metrics_callback is not None:\n",
    "            eval_metrics_callback(results, global_step.numpy())\n",
    "        metric_utils.log_metrics(eval_metrics)\n",
    "\n",
    "        time_step = None\n",
    "        policy_state = collect_policy.get_initial_state(tf_env.batch_size)\n",
    "\n",
    "        # Variables for logging time (steps_per_sec)\n",
    "        timed_at_step = global_step.numpy()\n",
    "        time_acc = 0  # Time accumulation\n",
    "\n",
    "        # Dataset\n",
    "        dataset = replay_buffer.as_dataset(\n",
    "            num_parallel_calls=3,\n",
    "            sample_batch_size=batch_size,\n",
    "            num_steps=train_sequence_length + 1,\n",
    "            single_deterministic_pass=False,\n",
    "        ).prefetch(3)\n",
    "        iterator = iter(dataset)\n",
    "\n",
    "        def train_step():\n",
    "            experience, _ = next(iterator)\n",
    "            return tf_agent.train(experience)\n",
    "\n",
    "        if use_tf_functions:\n",
    "            train_step = common.function(train_step)\n",
    "\n",
    "        for _ in range(num_iterations):\n",
    "            start_time = time.time()\n",
    "            time_step, policy_state = collect_driver.run(\n",
    "                time_step=time_step,\n",
    "                policy_state=policy_state,\n",
    "            )\n",
    "\n",
    "            for _ in range(train_steps_per_iteration):\n",
    "                train_loss = train_step()\n",
    "            time_acc += time.time() - start_time\n",
    "\n",
    "            if global_step.numpy() % log_interval == 0:\n",
    "                logging.info('step = %d, loss = %f', global_step.numpy(),\n",
    "                             train_loss.loss)\n",
    "                steps_per_sec = (global_step.numpy() - timed_at_step) / time_acc\n",
    "                logging.info('%.3f steps/sec', steps_per_sec)\n",
    "                tf.compat.v2.summary.scalar(\n",
    "                    name='global_steps_per_sec', data=steps_per_sec, step=global_step)\n",
    "\n",
    "                # Reset time.\n",
    "                timed_at_step = global_step.numpy()\n",
    "                time_acc = 0\n",
    "\n",
    "            for train_metric in train_metrics:\n",
    "                train_metric.tf_summaries(\n",
    "                    train_step=global_step, step_metrics=train_metrics[:2])\n",
    "\n",
    "            if global_step.numpy() % train_checkpoint_interval == 0:\n",
    "                train_checkpointer.save(global_step=global_step.numpy())\n",
    "            if global_step.numpy() % policy_checkpoint_interval == 0:\n",
    "                policy_checkpointer.save(global_step=global_step.numpy())\n",
    "            if global_step.numpy() % rb_checkpoint_interval == 0:\n",
    "                rb_checkpointer.save(global_step=global_step.numpy())\n",
    "\n",
    "            if global_step.numpy() % eval_interval == 0:\n",
    "                results = metric_utils.eager_compute(\n",
    "                    eval_metrics,\n",
    "                    eval_tf_env,\n",
    "                    eval_policy,\n",
    "                    num_episodes=num_eval_episodes,\n",
    "                    train_step=global_step,\n",
    "                    summary_writer=eval_summary_writer,\n",
    "                    summary_prefix='Metrics',\n",
    "                )\n",
    "                if eval_metrics_callback is not None:\n",
    "                    eval_metrics_callback(results, global_step.numpy())\n",
    "                metric_utils.log_metrics(eval_metrics)\n",
    "\n",
    "            # Record a video of current eval agent policy\n",
    "            if global_step.numpy() % video_recording_interval == 0:\n",
    "                filename = '{}_{}'.format(global_step.numpy(), get_timestamp())\n",
    "                full_filename = os.path.join(video_dir, filename)\n",
    "                create_policy_eval_video(\n",
    "                    eval_policy,\n",
    "                    eval_tf_env,\n",
    "                    eval_py_env,\n",
    "                    filename=full_filename,\n",
    "                    fps=15,\n",
    "                    freeze_seconds=3,\n",
    "                    num_episodes=1,\n",
    "                    step=global_step.numpy(),\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_filename = create_policy_eval_video(tf_agent.policy, eval_tf_env, eval_py_env, fps=15, freeze_seconds=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_mp4(saved_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
